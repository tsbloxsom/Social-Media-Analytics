{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Homework\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Choice', 'A_follower_count', 'A_following_count', 'A_listed_count',\n",
       "       'A_mentions_received', 'A_retweets_received', 'A_mentions_sent',\n",
       "       'A_retweets_sent', 'A_posts', 'A_network_feature_1',\n",
       "       'A_network_feature_2', 'A_network_feature_3', 'B_follower_count',\n",
       "       'B_following_count', 'B_listed_count', 'B_mentions_received',\n",
       "       'B_retweets_received', 'B_mentions_sent', 'B_retweets_sent', 'B_posts',\n",
       "       'B_network_feature_1', 'B_network_feature_2', 'B_network_feature_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5094545454545455"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['Choice'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = copy.deepcopy(train_raw[['Choice']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the amount of columns initially, we decided to subtract Bâ€™s values from A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['follower_count'] = train_raw['A_follower_count'] - train_raw['B_follower_count']\n",
    "train_df['following_count'] = train_raw['A_following_count'] - train_raw['B_following_count']\n",
    "train_df['listed_count'] = train_raw['A_listed_count'] - train_raw['B_listed_count']\n",
    "train_df['mentions_received'] = train_raw['A_mentions_received'] - train_raw['B_mentions_received']\n",
    "train_df['retweets_received'] = train_raw['A_retweets_received'] - train_raw['B_retweets_received']\n",
    "train_df['mentions_sent'] = train_raw['A_mentions_sent'] - train_raw['B_mentions_sent']\n",
    "train_df['retweets_sent'] = train_raw['A_retweets_sent'] - train_raw['B_retweets_sent']\n",
    "train_df['posts'] = train_raw['A_posts'] - train_raw['B_posts']\n",
    "train_df['network_feature_1'] = train_raw['A_network_feature_1'] - train_raw['B_network_feature_1']\n",
    "train_df['network_feature_2'] = train_raw['A_network_feature_2'] - train_raw['B_network_feature_2']\n",
    "train_df['network_feature_3'] = train_raw['A_network_feature_3'] - train_raw['B_network_feature_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network_feature_3  network_feature_3    1.000000\n",
       "network_feature_2  network_feature_2    1.000000\n",
       "follower_count     follower_count       1.000000\n",
       "following_count    following_count      1.000000\n",
       "listed_count       listed_count         1.000000\n",
       "mentions_received  mentions_received    1.000000\n",
       "retweets_received  retweets_received    1.000000\n",
       "mentions_sent      mentions_sent        1.000000\n",
       "retweets_sent      retweets_sent        1.000000\n",
       "posts              posts                1.000000\n",
       "network_feature_1  network_feature_1    1.000000\n",
       "Choice             Choice               1.000000\n",
       "retweets_received  mentions_received    0.988363\n",
       "mentions_received  retweets_received    0.988363\n",
       "retweets_received  network_feature_1    0.920574\n",
       "network_feature_1  retweets_received    0.920574\n",
       "                   mentions_received    0.914479\n",
       "mentions_received  network_feature_1    0.914479\n",
       "listed_count       follower_count       0.781208\n",
       "follower_count     listed_count         0.781208\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.corr().unstack().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we removed mentions_received and network_feature_1 (degree) because both of them were highly correlated with retweets_received. We then split our data into a train (75%) and test set and transformed all data to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# drop columns\n",
    "train_df = train_df.drop(columns=['mentions_received', 'network_feature_1'])\n",
    "y = train_df['Choice']\n",
    "X = train_df.drop(columns=['Choice'])\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 110)\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we trained a logistic regression model on the training set and tested it on the remaining data. The results as well as the confusion matrices are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.610310\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Choice   No. Observations:                 4125\n",
      "Model:                          Logit   Df Residuals:                     4116\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Tue, 11 Feb 2020   Pseudo R-squ.:                  0.1192\n",
      "Time:                        13:09:50   Log-Likelihood:                -2517.5\n",
      "converged:                       True   LL-Null:                       -2858.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                6.909e-142\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            11.0879      2.221      4.992      0.000       6.734      15.441\n",
      "x2            -0.6939      1.015     -0.684      0.494      -2.683       1.295\n",
      "x3            58.1284      4.824     12.050      0.000      48.674      67.583\n",
      "x4           -75.3840      4.148    -18.173      0.000     -83.514     -67.254\n",
      "x5             1.7545      0.564      3.110      0.002       0.649       2.860\n",
      "x6             1.9134      0.580      3.302      0.001       0.778       3.049\n",
      "x7             1.2067      0.631      1.911      0.056      -0.031       2.444\n",
      "x8             0.5439      0.800      0.680      0.497      -1.025       2.112\n",
      "x9             2.8584      0.741      3.857      0.000       1.406       4.311\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "logit = sm.Logit(y_train, X_train_scaled)\n",
    "result = logit.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set confustion matrix:\n",
      " [[1319  699]\n",
      " [ 488 1619]]\n",
      "\n",
      "Test set confustion matrix:\n",
      " [[451 229]\n",
      " [149 546]]\n",
      "\n",
      "Training set accuracy: 0.7122424242424242\n",
      "Test set accuracy: 0.7250909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_predictions(s):\n",
    "    if s >= 0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "train_predictions = pd.Series(result.predict(X_train_scaled))\n",
    "train_predictions = train_predictions.map(make_predictions)\n",
    "train = confusion_matrix(y_train, train_predictions)\n",
    "\n",
    "test_predictions = pd.Series(result.predict(X_test_scaled))\n",
    "test_predictions = test_predictions.map(make_predictions)\n",
    "test = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "print(\"Training set confustion matrix:\\n\", train)\n",
    "print(\"\\nTest set confustion matrix:\\n\", test)\n",
    "\n",
    "print(\"\\nTraining set accuracy:\", (train[0][0] + train[1][1])/(sum(train[0])+ sum(train[1])))\n",
    "print(\"Test set accuracy:\", (test[0][0] + test[1][1])/(sum(test[0])+ sum(test[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sorted the variables by the magnitude of their coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficients</th>\n",
       "      <th>p-values</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>retweets_received</td>\n",
       "      <td>-75.384021</td>\n",
       "      <td>8.481768e-74</td>\n",
       "      <td>75.384021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>listed_count</td>\n",
       "      <td>58.128448</td>\n",
       "      <td>1.944130e-33</td>\n",
       "      <td>58.128448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>follower_count</td>\n",
       "      <td>11.087859</td>\n",
       "      <td>5.983042e-07</td>\n",
       "      <td>11.087859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>network_feature_3</td>\n",
       "      <td>2.858370</td>\n",
       "      <td>1.147292e-04</td>\n",
       "      <td>2.858370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>retweets_sent</td>\n",
       "      <td>1.913395</td>\n",
       "      <td>9.614698e-04</td>\n",
       "      <td>1.913395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mentions_sent</td>\n",
       "      <td>1.754485</td>\n",
       "      <td>1.871340e-03</td>\n",
       "      <td>1.754485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>posts</td>\n",
       "      <td>1.206705</td>\n",
       "      <td>5.602249e-02</td>\n",
       "      <td>1.206705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>following_count</td>\n",
       "      <td>-0.693924</td>\n",
       "      <td>4.940719e-01</td>\n",
       "      <td>0.693924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>network_feature_2</td>\n",
       "      <td>0.543859</td>\n",
       "      <td>4.967619e-01</td>\n",
       "      <td>0.543859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   coefficients      p-values  magnitude\n",
       "retweets_received    -75.384021  8.481768e-74  75.384021\n",
       "listed_count          58.128448  1.944130e-33  58.128448\n",
       "follower_count        11.087859  5.983042e-07  11.087859\n",
       "network_feature_3      2.858370  1.147292e-04   2.858370\n",
       "retweets_sent          1.913395  9.614698e-04   1.913395\n",
       "mentions_sent          1.754485  1.871340e-03   1.754485\n",
       "posts                  1.206705  5.602249e-02   1.206705\n",
       "following_count       -0.693924  4.940719e-01   0.693924\n",
       "network_feature_2      0.543859  4.967619e-01   0.543859"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pd.DataFrame({'coefficients': result.params.values, 'p-values': result.pvalues.values}, index = X_train.columns)\n",
    "out['magnitude'] = out['coefficients'].abs()\n",
    "out.sort_values(by = ['magnitude'], ascending = False, inplace = True)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we took the top four important variables and scaled them to where they sum to 1. These are the variables and weights we use in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweets_received   -22.779148\n",
       "listed_count         17.564949\n",
       "follower_count        3.350471\n",
       "network_feature_3     0.863727\n",
       "Name: coefficients, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['coefficients'].iloc[:4] / abs(out['coefficients'].iloc[:4].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our model, the best predictors of influence are retweets_received, listed_count, and follower_count. A surprise is that retweet_ received is negative, meaning that more retweets are related to a lesser influencer status. On the other hand, listed_count and follower_count are both positive, which make sense. A business could use these results by focusing primarily on how many followers their marketers have to identify who will be more influential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial Value Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected return without analytics:**\n",
    "\n",
    "0.01% * $10/follower - 2 * $5 = $0.10/follower - $10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected return with our analytics:**\n",
    "\n",
    "72.5% * 0.015% * $10/follower - $10 = $0.10875/follower - $10\n",
    "\n",
    "**Increase over no analytics: $0.00875/follower**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect analytical model:**\n",
    "\n",
    "100% * 0.015% * $10/follower - $10 = $0.15/follower - $10\n",
    "\n",
    "\n",
    "**Increase over no analytics: $0.05/follower**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect model adds 5 cents per follower over doing no analytics compared to our model which adds 0.875 cents per follower. Therefore it becomes clear that tweaking this model further to be more accurate could further increase the financial value of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
